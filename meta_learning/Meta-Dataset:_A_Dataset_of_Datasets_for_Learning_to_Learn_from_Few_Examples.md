# Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples by Triantafillou et al. (2019)

[Link to paper](https://arxiv.org/abs/1903.03096)

## General Notes

* This paper proposes a novel dataset for evaluation of algorithms for few-shot classification of images. The authors believe that the current benchmarks (Omniglot, mini-Imagenet) are approaching their limit in terms of measuring the success of different approaches. First, the most recent methods perform similarly on these datasets when controlling for model capacity. Second, they are homogeneous (fixed number of shots and examples per class), which contrasts with real-life experiences. Next, they do not present challenging generalization to new distributions. Finally, they also ignore relationships between classes when forming episodes (e.g., having two dog breeds as classes is different than having a dog class and a helicopter class);

## Method

Data for the Meta-Dataset comes from 10 different datasets. This allows for evaluation in a new dataset altogether, which is a challenging generalization problem. For example, some of the datasets are:

* Omniglot: a dataset of thousands of different digits, with 20 instances per digit;
* mini-Imagenet: a dataset of 100 Imagenet classes, with 600 examples per class;
* Aircraft: a dataset of images of aircraft with 102 models and 100 images per model;
* Fungi: a dataset of approximately 100 thousand images of nearly 1,500 wild mushrooms species;
* Traffic Signs: A dataset of 50,000 images of German road signs in 43 classes;

Each episode generated by Meta-Dataset uses classes from a single dataset to ensure that it is a realistic classification problem. Two datasets are used only for evaluation (MSCOCO and Traffic Signs), not appearing in the training set. The remaining datasets contribute with classes to the training, validation, and test sets in a roughly 70%/15%/15% proportion, respectively. Aside from this, the Omniglot and mini-Imagenet datasets have some class structure exploited in the construction of the episodes (the classes present in the validation/test sets are similar, e.g., carnivores for mini-Imagenet).

Next, the authors define an episode sampling procedure for the Meta-Dataset. It consists of the following steps:

* **Step 1: Sample the episode's class set** For datasets without hierarchy, the number of classes is sampled uniformly in the interval $[5, M]$, where $M$ is the maximum number of classes. For the remaining datasets, an internal node of the hierarchy DAG is sampled, and the leaves that are descendants of the sampled node (or a subset of them, if there are too many) are chosen as the episode's classes. This introduces variability to the relation between classes (when the sampled internal node is close to the root, we have a coarse classification episode; otherwise, it is more fine-grained).

* **Step 2: Sample the episode's examples** For each class, choose the examples that will populate it to form an episode. This process can be broken down into three steps:

* **Step 2a: Compute the query set size**: The query set (also called test set) is class-balanced. The number of query images per class is such that all classes have enough images to contribute and remain with at least half of its examples to add to the support set possibly. This number is capped at 10;

* **Step 2b: Compute the support set size**: The support set (also called training set) has at most 500 examples. Each class contributes at most 100 of its remaining examples. The number of available examples (for all classes) is multiplied by a scalar sampled uniformly from the interval $[0, 1]$ to enable the generation of episodes with few examples, even though many images are available. Each class must have at least one image in the support set;

* **Step 2c: Compute the shot of each class**: The shot (number of examples) of each class is computed by distributing the total support set size among all classes, ensuring that each class has at least one image on the support set. The proportions are a noisy version of the normalized number of images for that class in the dataset. This design decision is made in the hopes of obtaining realistic class ratios.

After this, the specified number of images are sampled at random from the examples of each class to construct the episode.

Aside from the Meta-Dataset, this paper introduces a novel architecture for few-shot classification, combining MAML and Prototypical Networks, which learn the weights of the last layer from Prototypical Networks via the optimization procedure of MAML. The variant of MAML used is the first-order approximation both in this novel method and in the MAML baseline, which was chosen due to the memory requirements of the vanilla MAML approach.

## Results

The paper compares the results of several baselines on the Meta-Dataset. The first two baselines are trained on the entire training set at once with an output unit per class. Then, the final layer is removed, and the last hidden units are used as a learned embedding for the $k$-NN and Baseline++ approaches. Aside from these baselines, the authors also employ matching networks, prototypical networks, relation networks, and MAML-based models, in addition to their novel Proto-MAML approach. Proto-MAML outperforms the other baselines until the average number of images per class increases.

The validation set consists of only mini-Imagenet classes, hopefully being a good proxy for general performance. The authors also explored initializing the network weights to those in the $k$-NN baseline pretrained on the Imagenet dataset to convergence. This initialization was found to be generally beneficial not only to the mini-Imagenet tasks but to all datasets. The datasets with natural images benefited the most from the initialization, while it underperformed on significantly different datasets such as Omniglot.

The effect of training on all datasets instead of only training in the mini-Imagenet was a significant gain in accuracy on datasets that are reasonably different from Imagenet, such as Omniglot. However, little is gained on the other datasets, suggesting that our current methods cannot exploit heterogeneous data sources to improve generalization to unseen classes of diverse origins.

The authors also studied the benefits of meta-learning by training baseline models (such as prototypical networks) in a non-episodic fashion but evaluated episodically using the inference algorithm of the meta-learner. These inference baselines are strong: there is a small benefit to meta-learning when training on mini-Imagenet only, usually disappearing when training on all datasets (even hurting performance in some cases). This result implies that the current meta-learning methods are not fit to learn from diverse data sources effectively.

## Details

* The results section is exciting because it decouples a few different elements of the experiments: pretraining, episode-based training, training on all datasets or only one, etc.
