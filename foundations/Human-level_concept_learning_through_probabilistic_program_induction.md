# Human-level concept learning through probabilistic program induction by Lake et al. (2015)

[Link to paper](https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf)

* Humans are able to learn from one or few examples, most modern deep learning algorithms cannot;

* People learn richer representations than machines and use them for a wide range of functions such as classification, generation, parsing objects into parts and creating abstract categories;

* The goal is then to learn rich concepts from sparse data, navigating the trade-off of more complicated models needing more data;

* Bayesian Program Learning is a framework in which probabilistic generative models are expressedas structured procedures in an abstract description;

* Three key ideas: compositionality, causality and learning to learn;

* Probabilistic components support generalization, procedural from naturally captures the "causal" structure of real-world processes and a learned inductive bias serves as the meta-learning component;

## Bayesian Program Learning

* Learns simple stochastic processes to represent concepts, building them compositionally from parts, subparts and spatial relations;

* A generative model for generative models.

## Details

* Some of the concepts and figures from this paper are featured on Hinton's Keynote on AAAI 2020;

